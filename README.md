# Pseudo-SD (ICCV25) 
(ICCV25) Pseudo-SD: Pseudo Controlled Stable Diffusion for Semi-Supervised and Cross-Domain Semantic Segmentation


This is a pytorch implementation of our paper Pseudo-SD.

## Start

Please download the pre-trained Stable Diffusion model
```
mkdir models/ldm/stable-diffusion
wget -O models/ldm/stable-diffusion/sd-v1-4-full-ema.ckpt https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt
```

Dataset preparation : Store the dataset path in json file.


## Training

To train Pseudo-SD, run the script [](./sample_Cityscapes_pseudo_balance_rate_clasee.sh)
```shell
CUDA_VISIBLE_DEVICES=0 nohup python main.py --base configs/stable-diffusion/v1-finetune_Cityscapes.yaml \
                                      -t \
                                      --actual_resume models/ldm/stable-diffusion/sd-v1-4-full-ema.ckpt \
                                      -n exp_cityscapes_masked_pseudo_rate_class \
                                      --gpus 0, \
                                      --data_root /data/UniMatch-main/dataset/Cityscapes/ \
                                      --train_txt_file /data/splits/cityscapes/DTST_DIFF/rate_class.txt \
                                      --val_txt_file /data/DTST/datasets/cityscapes_val_list.txt \
				      > logs/cityscapes_train_pseudo.file 2>&1 &
```
If you want to train with a custom dataset
1. Generate a [json file](./data/SECOND_train_0.8.json) for the custom dataset with its path information.
2. You can modify the [dataloader file](./ldm/data/SECOND.py)

    Modify the dataset information such as label_mapping, label_palette, data_dict and dataset path information in the dataloader file.
3. Modify the configuration file

    Modify data.train.target and data.validation.target in the [configuration file](./configs/stable-diffusion/v1-finetune_SECOND.yaml) to the contents of the dataloader.
4. Replace the path of the json file and the modified configuration file into the runtime parameters.

Then, you can use the customized dataset for training


## Generation

Before generating images, [a json file](./data/sample_4.json) containing layout path information generated by T2I model is needed.

To generate images using L2I, run the script [](./sample_SECOND_AB.sh)
```shell
CUDA_VISIBLE_DEVICES=1 nohup python LIS.py --batch_size 10 \
                                           --out_num 1000 \
                                             --config configs/stable-diffusion/v1-finetune_Cityscapes.yaml \
					     --ckpt logs/exp_cityscapes_masked_pseudo_rate_class/checkpoints/last.ckpt \
                                             --dataset CityscapesBalance \
					     --outdir outputs/Cityscapes_LIS_mask_pseudo_balance_rate_class \
                                             --txt_file /data/1_2/1_2.p  \
                                             --data_root /data/seco/ \
                                             --plms > logs/LIS_hard_pseudo_balance_rate_class.logs 2>&1 &
```

If you want to generate images using weights trained on a custom dataset
1. Modify the dataset parameter in the run parameter to the name of the customized dataset
2. Add the information of the customized dataset in the [inference file](./LIS_AB.py)

    Like `elif opt.dataset == "ADE20K"` and the code that follows it.
3. Modify the runtime parameters in the inference script such as ckpt, outdir, and txt_file.

Then you can get the results of the custom dataset generation


## Acknowledgments
Our code borrows heavily from [FreestyleNet](https://github.com/essunny310/FreestyleNet)

